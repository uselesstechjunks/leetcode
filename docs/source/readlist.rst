Reading List
##############################

Ilya's Paper List
***********************************
* Fundamentals

	* The Annotated Transformer
	* The First Law of Complexodynamics
	* The Unreasonable Effectiveness of RNNs
	* Understanding LSTM Networks
	* Recurrent Neural Network Regularization
	* Keeping Neural Networks Simple by Minimizing the Description Length of the Weights
	* Pointer Networks
	* ImageNet Classification with Deep CNNs
	* Order Matters: Sequence to Sequence for Sets
	* GPipe: Efficient Training of Giant Neural Networks
	* Deep Residual Learning for Image Recognition
	* Multi-Scale Context Aggregation by Dilated Convolutions
	* Neural Quantum Chemistry
	* Attention Is All You Need
	* Neural Machine Translation by Jointly Learning to Align and Translate
	* Identity Mappings in Deep Residual Networks
	* A Simple NN Module for Relational Reasoning
	* Variational Lossy Autoencoder
	* Relational RNNs
	* Quantifying the Rise and Fall of Complexity in Closed Systems
	* Neural Turing Machines
	* Deep Speech 2: End-to-End Speech Recognition in English and Mandarin
	* Scaling Laws for Neural LMs (arxiv.org)
	* A Tutorial Introduction to the Minimum Description Length Principle (arxiv.org)
	* Machine Super Intelligence Dissertation (vetta.org)
	* PAGE 434 onwards: Komogrov Complexity (lirmm.fr)
	* CS231n Convolutional Neural Networks for Visual Recognition (cs231n.github.io)

* Additional Resources

	* `arc.net/folder <https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE>`_
	* [GPT-1](https://lnkd.in/gJ5Pe3HG)
	* [GPT-2](https://lnkd.in/gatQi8Ud)
	* [GPT-3](https://lnkd.in/g43GzYfZ)
	* [GPT-4](https://lnkd.in/ga_xEpEj)
	* [Llama-2](https://lnkd.in/gutaGW8h)
	* [Tools](https://lnkd.in/gqJ3aXpS)
	* [Gemini-Pro-1.5](https://lnkd.in/gbDcYp89)
	* [Agentic Patterns Series](https://lnkd.in/gphZ6Y5s)

Stanford courses
***********************************
* Natural Language Processing - https://web.stanford.edu/class/cs224n/
* Transformers United - https://web.stanford.edu/class/cs25/
* Deep Multi-task and Meta Learning - https://cs330.stanford.edu/
* Machine Learning with Graphs - https://web.stanford.edu/class/cs224w/
* Convolutional Neural Networks for Visual Recognition - https://cs231n.github.io/
* Deep generative models - https://deepgenerativemodels.github.io/
* Systems for Machine Learning - https://cs229s.stanford.edu/fall2023/
* Stanford MLSys Seminar - https://mlsys.stanford.edu/
* Mining Massive Dataset - https://web.stanford.edu/class/cs246/

Must Read Papers
***********************************
.. csv-table:: 
	:header: "Tag", "Title", "Link"
	:align: center
	
		Transformer,Attention Is All You Need,
		MLM, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,
		MLM, RoBERTa: A Robustly Optimized BERT Pretraining Approach,
		MLM, TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Efficient Retrieval,
		CLM, GPT2: Language Models are Unsupervised Multitask Learners,
		CLM, GPT3: Language Models are Few-Shot Learners,
		CLM, LLaMA: Open and Efficient Foundation Language Models,
		CLM, Llama 2: Open Foundation and Fine-Tuned Chat Models,
		CLM, Mixtral: Mixtral of Experts,
		PLM, XLNet: Generalized Autoregressive Pretraining for Language Understanding,
		Seq2Seq, BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation Translation and Comprehension,
		Seq2Seq, T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,
		Multilingual, XLM: Cross-lingual Language Model Pretraining,
		Multilingual, XLM-R: Unsupervised Cross-lingual Representation Learning at Scale,
		Multilingual, mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,
		Efficiency, LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,
		Efficiency, FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness,
		Efficiency, LoRA Low-Rank Adaptation of Large Language Models,
		Efficiency, QLORA: Efficient Finetuning of Quantized LLMs,
		LLM, InstructGpt: Training language models to follow instructions with human feedback,
		LLM, PPO: Proximal Policy Optimization Algorithms,
		LLM, SFT+RLHF: Learning to summarize from human feedback,
		LLM, DPO: Direct Preference Optimization: Your Language Model is Secretly a Reward Model,

Linear Algebra
***********************************

* [3Blue1Brown] `Essence of linear algebra <https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab>`_
* [MIT] `18.065 - Matrix Methods for Data Analysis <https://www.youtube.com/playlist?list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k>`_

Calculus
***********************************

* [3Blue1Brown] `Essence of calculus <https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr>`_
* [Khan Academy] `Multivariable calculus <https://www.khanacademy.org/math/multivariable-calculus>`_
* [University of Victoria] `MATH200: Calculus III: Multivariable Calculus <https://www.youtube.com/playlist?list=PLHXZ9OQGMqxc_CvEy7xBKRQr6I214QJcd>`_

Probability & Statistics
***********************************

* [MIT] `RES.6-012 Introduction to Probability <https://www.youtube.com/playlist?list=PLUl4u3cNGP60hI9ATjSFgLZpbNJ7myAg6>`_
* [CMU] `36-705 - Intermediate Statistics <https://www.youtube.com/playlist?list=PLt2Pd5kunvJ6-wpJG9hlWlk47c76bm9L6>`_
* [statisticsmatt] `Introduction to Mathematical Statistics with Illustrations using R <https://www.youtube.com/playlist?list=PLmM_3MA2HWpan-KlYp-QCbPHxMj5FK0TB>`_
* `Mathematician uncovers methods to shrink sampling errors in large-dimensional data sets <https://phys.org/news/2023-03-mathematician-uncovers-methods-sampling-errors.html>`_

Analysis
***********************************

* [SO] `Pointwise vs. Uniform Convergence <https://math.stackexchange.com/questions/597765/pointwise-vs-uniform-convergence#915867>`_

Complex Analysis
***********************************

* `Visual Complex Analysis <https://complex-analysis.com/content/table_of_contents.html>`_

ML Theory
***********************************

* [Goodfellow] `Deep Learning <https://www.deeplearningbook.org/>`_
* [Roberts] `The Principles of Deep Learning Theory <https://arxiv.org/abs/2106.10165>`_
* [Kevin Murphy] `Probabilistic Machine Learning book1 <https://probml.github.io/pml-book/book1.html>`_
* [Kevin Murphy] `Probabilistic Machine Learning book2 <https://probml.github.io/pml-book/book2.html>`_
* [Bronstein,Bruna,Cohen,Veickovic][2021] `Geometric Deep Learning <https://geometricdeeplearning.com/>`_
* [Shwartz David] `Understanding Machine Learning - From Theory to Algorithms <https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf>`_
* [Mohri] `Foundations of Machine Learning <https://cs.nyu.edu/~mohri/mlbook/>`_
* [CMU] `11-785 Deep Learning <https://www.youtube.com/playlist?list=PLp-0K3kfddPxRmjgjm0P1WT6H-gTqE8j9>`_
* `Yet Another Derivation of Backpropagation in Matrix Form <https://sudeepraja.github.io/BackpropAdjoints/>`_
* `Gradients are Not All You Need <https://arxiv.org/pdf/2111.05803.pdf>`_
* `The Decade of Deep Learning <https://bmk.sh/2019/12/31/The-Decade-of-Deep-Learning/>`_
* `Long-Tailed Learning Requires Feature Learning <https://openreview.net/pdf?id=S-h1oFv-mq>`_
* `[MIT] Statistical Learning Theory and Applications <https://cbmm.mit.edu/lh-9-520/syllabus>`_
* `[GPSS] Gaussian Process Summer School <https://gpss.cc/gpss23/program>`_

ML Practical
***********************************

* [Andrej Karpathy] `Neural Networks: Zero to Hero <https://karpathy.ai/zero-to-hero.html>`_
* `pytorch-internals <http://blog.ezyang.com/2019/05/pytorch-internals/>`_
* https://forums.fast.ai/t/diving-deep-into-pytorch/39470
* [Stevens] `Deep Learning with PyTorch <https://www.manning.com/books/deep-learning-with-pytorch>`_
* [Geron] `Hands-on Machine Learning <https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/>`_
* [Howard] `Deep Learning for Coders with fastai and PyTorch <https://course.fast.ai/Resources/book.html>`_
* [Zheng Casari] Feature Engineering for Machine Learning
* [NYU] `Deep Learning (Spring 2020) <https://atcold.github.io/pytorch-Deep-Learning/>`_
* [CMU] `Dive into Deep Learning <https://d2l.ai/index.html>`_
* [MIT] `6.S965 TinyML and Efficient Deep Learning <https://efficientml.ai/>`_
* [Microsoft Research] `LMOps <https://github.com/microsoft/LMOps>`_
* `Data Centric AI Cource <https://github.com/dcai-course/dcai-course>`_


ML Design General Principle
***********************************

* [Andrew Ng] `Machine Learning Yearning <https://www.mlyearning.org/>`_
* [Chip Huyen] Designing Machine Learning Systems
* [Burkov] Machine Learning Engineering
* [Jeff Smith] Machine Learning Systems
* [Lakshmanan] Machine Learning Design Patterns
* [UCB] System Design for Large Scale Machine Learning


ML Math
***********************************

* [Gutmann] Pen and Paper Exercise in ML
* `Steve Brunton Playlist <https://www.youtube.com/@Eigensteve/playlists>`_
* `Matrix Calculus <https://www.matrixcalculus.org/>`_


ML Algorithms
***********************************

* [Naumann] The Art of Differentiating Computer Programs


ML Related Theory
***********************************

* [MacKay] Information Throry Inference and Learning Algorithms
* [Brunton Kutz] Data Driven Science and Engineering
* [CUP] Probabilistic Numerics
* [Easley Kleinberg] Networks Crowds and Markets - Reasoning About a Highly Connected World


Applied ML
***********************************

* [Liu] Learning to Rank for Information Retrieval
* [MSR] A Short Introduction to Learning to Rank
* [MSR] LambdaMART
* [Ravichandiran] Getting Started with Google BERT
* [101ai.net] `BERT Explorer <https://www.101ai.net/text/bert>`_
* [Rothman] Transformers for Natural Language Processing
* [Tunstall] Natural Language Processing with Transformers
* [lilianweng] `The Transformer Family Version 2.0 <https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/>`_
* [Lakshmanan] Practical Machine Learning for Computer Vision
* Recent Advances and Trends in Multimodal Deep Learning
* Recommender Systems
* [Stanford] `CS224n: Natural Language Processing with Deep Learning <https://web.stanford.edu/class/cs224n/index.html>`_
* [Stanford] `CS224U - Natural Language Understanding <https://www.youtube.com/playlist?list=PLoROMvodv4rPt5D0zs3YhbWSZA8Q_DyiJ>`_
* [Stanford] `CS25 - Transformers United <https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM>`_
* [Stanford] `CS330 - Deep Multi-Task and Meta-Learning <https://www.youtube.com/playlist?list=PLoROMvodv4rMIJ-TvblAIkw28Wxi27B36>`_
* `From Deep to Long Learning? <https://hazyresearch.stanford.edu/blog/2023-03-27-long-learning>`_
* [CMU] `Graham Neubig's Teaching <https://www.phontron.com/teaching.php>`_
* [Princeton] `Against Predictive Optimization <https://predictive-optimization.cs.princeton.edu/>`_
* [Github] Must Read Papers on Pre-Training <https://github.com/thunlp/PLMpapers>`_
* `NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers <https://speechresearch.github.io/naturalspeech2/>`_


LLMs
***********************************

* `[Excellent Blog Post by Sebastian Rachka] Understanding Large Language Models <https://magazine.sebastianraschka.com/p/understanding-large-language-models>`_
* `[Github] LLM Course <https://github.com/mlabonne/llm-course>`_
* `Generative Agents: Interactive Simulacra of Human Behavior <https://arxiv.org/pdf/2304.03442.pdf>`_
* `Locating and Editing Factual Associations in GPT <https://arxiv.org/pdf/2202.05262.pdf>`_
* `Jarvis/HuggingGPT <https://github.com/microsoft/JARVIS>`_
* `Sparks of Artificial General Intelligence <https://arxiv.org/pdf/2303.12712.pdf>`_
* `Reflexion: an autonomous agent with dynamic memory and self-reflection <https://arxiv.org/pdf/2303.11366.pdf>`_
* [MIT] `Self-Supervised Learning and Foundation Models <https://www.futureofai.mit.edu/>`_
* [Stanford] `HELM - Holistic Evaluation of Language Models <https://crfm.stanford.edu/helm/latest/>`_
* `Transformer Math 101 <https://blog.eleuther.ai/transformer-math/>`_
* `Large Language Models: Scaling Laws and Emergent Properties <https://cthiriet.com/articles/scaling-laws>`_
* `Hyena Hierarchy: Towards Larger Convolutional Language Models <https://arxiv.org/pdf/2302.10866.pdf>`_
* `Scaling Transformer to 1M tokens and beyond with RMT <https://arxiv.org/pdf/2304.11062.pdf>`_
* `AI/ML/LLM/Transformer Models Timeline and List <https://ai.v-gar.de/ml/transformer/timeline/>`_
* `Think Before You Act: Unified Policy for Interleaving Language Reasoning with Actions <https://arxiv.org/pdf/2304.11063.pdf>`_


Applied LLMs
***********************************

* `[Github] LLM4Rec: Collection of papers <https://github.com/WLiK/LLM4Rec-Awesome-Papers>`_
* `Freepik - A New Search for the New World <https://www.freepik.com/blog/new-search-new-world/>`_
* `Replacing my best friends with an LLM <https://www.izzy.co/blogs/robo-boys.html>`_
* `Become a 1000x engineer or die tryin <https://kadekillary.work/posts/1000x-eng/>`_
* `Man and machine: GPT for second brains <https://reasonabledeviations.com/2023/02/05/gpt-for-second-brain/>`_
* `Learn Prompting <https://learnprompting.org/>`_
* `Prompt Engineering vs. Blind Prompting <https://mitchellh.com/writing/prompt-engineering-vs-blind-prompting>`_
* `An example of LLM prompting for programming <https://martinfowler.com/articles/2023-chatgpt-xu-hao.html>`_
* `Chat with any PDF <https://www.chatpdf.com/>`_
* `AI prompt-to-storyboard videos w/ GPT, Coqui voices, StabilityAI images <https://meyer.id/>`_
* `ChatGPT for your site <https://letterdrop.com/chatgpt?ref=hn>`_
* `Web LLM runs the vicuna-7b Large Language Model entirely in your browser <https://simonwillison.net/2023/Apr/16/web-llm/>`_
* [Harvard] CS50 Tech Talk: `GPT-4 - How does it work, and how do I build apps with it? <https://www.youtube.com/watch?v=vw-KWfKwvTQ>`_

ML Papers
***********************************

* [dair-ai] `ML-Papers-Explained <https://github.com/dair-ai/ML-Papers-Explained>`_
* `Transformer models: an introduction and catalog — 2023 Edition <https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/>`_
* [Meta AI] `Teaching AI advanced mathematical reasoning <https://ai.facebook.com/blog/ai-math-theorem-proving/?utm_campaign=evergreen&utm_source=linkedin&utm_medium=organic_social&utm_content=blog>`_
* [Microsoft Research] `Why Can GPT Learn In-Context? <https://arxiv.org/pdf/2212.10559v2.pdf>`_
* [HM] `ML papers to implement <https://news.ycombinator.com/item?id=34503362>`_
* [ICLR2023] `Diffusion Models already have a Semantic Latent Space <https://arxiv.org/pdf/2210.10960.pdf>`_


MLE Papers
***********************************

* [ACM] DNN for YouTube Recommendations
* [FB] Local Search
* [FB] Photo Search
* [FB] Recommeding items to more than a billion people
* [ICML] ScaNN
* [NeurIPS] DiskANN
* [KDD] Predicting Clicks on Ads at Facebook
* [RecSys] Recommending What Video to Watch Next
* `91% of ML Models Degrade in Time <https://www.nannyml.com/blog/91-of-ml-perfomance-degrade-in-time>`_


MLOps
***********************************

* `The big dictionary of MLOps <https://www.hopsworks.ai/mlops-dictionary>`_


ML Interviews
***********************************

* [Kashan] Deep Learning Interviews


System Design General Principles
***********************************

* [Kleppmann] Designing Data-Intensive Applications
* [Alex Xu] System Design Interview - An Insiders Guide
* [Alex Xu] System Design Interview - An Insider's Guide Volume 2
* [Donne Matrin] `System Design Primer <https://github.com/donnemartin/system-design-primer>`_
* [Binh Nguyen] `Awesome Scalability <https://github.com/binhnguyennus/awesome-scalability>`_
* [Educative] `Grokking Modern System Design Interview for Engineers & Managers <https://www.educative.io/courses/grokking-modern-system-design-interview-for-engineers-managers>`_
* `A Senior Engineer's Guide to System Design Interview <https://interviewing.io/guides/system-design-interview>`_


System Design Algorithms
***********************************

* [Gakhov] Probabilistic Data Structures and Algorithms
* [Tyler Neylon] `Introduction to Locality-Sensitive Hashing <https://tylerneylon.com/a/lsh1/lsh_post1.html>`_


System Design Practical
***********************************

* `Build Your Own Redis with C/C++ <https://build-your-own.org/redis/>`_
* `Build Your Own Database <https://build-your-own.org/blog/20230420_byodb_done/>`_
* `The Inner Workings of Distributed Databases <https://questdb.io/blog/inner-workings-distributed-databases/>`_


Layoffs
***********************************

* `Effective Immediately <https://github.com/Effective-Immediately/effective-immediately>`_


Misc
***********************

* `Sampling - Interesting post on LinkedIn <https://www.linkedin.com/posts/sahil0094_sampling-trainingdata-machinelearnig-activity-7043559310324285440-58h2>`_
* [Developer-Y] `CS Video Courses <https://github.com/Developer-Y/cs-video-courses>`_
* `Openintro Statistics <https://www.openintro.org/book/os/>`_
* `Demystifying Fourier analysis <https://dsego.github.io/demystifying-fourier/>`_
* `Data-oriented Programming in Python <https://www.moderndescartes.com/essays/data_oriented_python/>`_
* [CMU] `15-751 CS Theory Toolkit <https://www.youtube.com/playlist?app=desktop&list=PLm3J0oaFux3ZYpFLwwrlv_EHH9wtH6pnX>`_
* `Data Structure Sketches <https://okso.app/showcase/data-structures>`_
* [HN] `Vectors are over, hashes are the future <https://news.ycombinator.com/item?id=33123972>`_
* `Tensor Search <https://www.reddit.com/r/MachineLearning/comments/xk31n8/p_my_cofounder_and_i_quit_our_engineering_jobs_at/>`_
* `Philosophy of Mathematics - A Readinng List <https://www.logicmatters.net/2020/11/16/philosophy-of-mathematics-a-reading-list/>`_
* `The faker's guide to reading (x86) assembly language <https://www.timdbg.com/posts/fakers-guide-to-assembly/>`_
* `Learn C++ <https://www.learncpp.com/>`_
* `Introducing Austral: A Systems Language with Linear Types and Capabilities <https://borretti.me/article/introducing-austral>`_
* `A Beautiful Mathematical Reading List for 2023 <https://abakcus.com/a-beautiful-mathematical-reading-list-for-2023/>`_
* `Vector Animations With Python <https://zulko.github.io/blog/2014/09/20/vector-animations-with-python/>`_
* `Systems design 2: What we hope we know <https://apenwarr.ca/log/20230415>`_
* `Irregular Expressions <https://tavianator.com/2023/irregex.html>`_
* `The Prospect of an AI Winter <https://www.erichgrunewald.com/posts/the-prospect-of-an-ai-winter/>`_
* `When Will AI Take Your Job? <https://unchartedterritories.tomaspueyo.com/p/when-will-ai-take-your-job>`_
* `What Is Disruptive Innovation? <https://hbr.org/2015/12/what-is-disruptive-innovation>`_
* `Category Theory ∩ Machine Learning <https://github.com/bgavran/Category_Theory_Machine_Learning>`_
* `Building a Better World without Jobs <https://workforcefuturist.substack.com/p/building-a-better-world-without-jobs-video>`_
* `The Joy of Abstraction - An Introduction to Category Theory <https://johncarlosbaez.wordpress.com/2023/02/11/the-joy-of-abstraction/>`_
* `Clean Code - Horrible Performance <https://www.computerenhance.com/p/clean-code-horrible-performance>`_
* `Reverse Engineering a Mysterious UDP stream in my hotel <https://www.gkbrk.com/2016/05/hotel-music/>`_
* `Procrastinating is linked to health and career problems <https://theconversation.com/procrastinating-is-linked-to-health-and-career-problems-but-there-are-things-you-can-do-to-stop-188322>`_
* `Map of Reddit <https://anvaka.github.io/map-of-reddit/?v=2>`_
* `The Embedding Archives: Millions of Wikipedia Article Embeddings in Many Languages <https://txt.cohere.com/embedding-archives-wikipedia/>`_
* `Why Oatmeal is Cheap: Kolmogorov Complexity and Procedural Generation <https://knivesandpaintbrushes.org/projects/why-oatmeal-is-cheap/why_oatmeal_is_cheap_fdg2023.pdf>`_
* `Blog: Haskell in Production <https://serokell.io/blog/haskell-in-production>`_
* `How Does an FPGA Work? <https://learn.sparkfun.com/tutorials/how-does-an-fpga-work/all>`_
