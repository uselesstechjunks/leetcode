##########################################################################################
Reading List
##########################################################################################

******************************************************************************************
Ilya's Paper List
******************************************************************************************
* `arc.net/folder <https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE>`_

Fundamentals
------------------------------------------------------------------------------------------
* The Annotated Transformer
* The First Law of Complexodynamics
* The Unreasonable Effectiveness of RNNs
* Understanding LSTM Networks
* Recurrent Neural Network Regularization
* Keeping Neural Networks Simple by Minimizing the Description Length of the Weights
* Pointer Networks
* ImageNet Classification with Deep CNNs
* Order Matters: Sequence to Sequence for Sets
* GPipe: Efficient Training of Giant Neural Networks
* Deep Residual Learning for Image Recognition
* Multi-Scale Context Aggregation by Dilated Convolutions
* Neural Quantum Chemistry
* Attention Is All You Need
* Neural Machine Translation by Jointly Learning to Align and Translate
* Identity Mappings in Deep Residual Networks
* A Simple NN Module for Relational Reasoning
* Variational Lossy Autoencoder
* Relational RNNs
* Quantifying the Rise and Fall of Complexity in Closed Systems
* Neural Turing Machines
* Deep Speech 2: End-to-End Speech Recognition in English and Mandarin
* Scaling Laws for Neural LMs (arxiv.org)
* A Tutorial Introduction to the Minimum Description Length Principle (arxiv.org)
* Machine Super Intelligence Dissertation (vetta.org)
* PAGE 434 onwards: Komogrov Complexity (lirmm.fr)
* CS231n Convolutional Neural Networks for Visual Recognition (cs231n.github.io)

Additional Resources
------------------------------------------------------------------------------------------
* [GPT-1](https://lnkd.in/gJ5Pe3HG)
* [GPT-2](https://lnkd.in/gatQi8Ud)
* [GPT-3](https://lnkd.in/g43GzYfZ)
* [GPT-4](https://lnkd.in/ga_xEpEj)
* [Llama-2](https://lnkd.in/gutaGW8h)
* [Tools](https://lnkd.in/gqJ3aXpS)
* [Gemini-Pro-1.5](https://lnkd.in/gbDcYp89)
* [Agentic Patterns Series](https://lnkd.in/gphZ6Y5s)

******************************************************************************************
University courses
******************************************************************************************
* [Stanford] Natural Language Processing - https://web.stanford.edu/class/cs224n/
* [Stanford] Transformers United - https://web.stanford.edu/class/cs25/
* [Stanford] Deep Multi-task and Meta Learning - https://cs330.stanford.edu/
* [Stanford] Machine Learning with Graphs - https://web.stanford.edu/class/cs224w/
* [Stanford] Convolutional Neural Networks for Visual Recognition - https://cs231n.github.io/
* [Stanford] Deep generative models - https://deepgenerativemodels.github.io/
* [Stanford] Systems for Machine Learning - https://cs229s.stanford.edu/fall2023/
* [Stanford] Stanford MLSys Seminar - https://mlsys.stanford.edu/
* [Stanford] Mining Massive Dataset - https://web.stanford.edu/class/cs246/
* [CMU] Large Language Model Systems - https://llmsystem.github.io/llmsystem2024spring/

******************************************************************************************
LLMs
******************************************************************************************
.. note::
	* [Practical] `The Large Language Model Playbook <https://cyrilzakka.github.io/llm-playbook/index.html>`_
	* [Sebastian Rachka] `Understanding Large Language Models <https://magazine.sebastianraschka.com/p/understanding-large-language-models>`_
	* [Github] `LLM Course <https://github.com/mlabonne/llm-course>`_
	* [MIT] `Self-Supervised Learning and Foundation Models <https://www.futureofai.mit.edu/>`_
	* [Stanford] `HELM - Holistic Evaluation of Language Models <https://crfm.stanford.edu/helm/latest/>`_
	* `Transformer Math 101 <https://blog.eleuther.ai/transformer-math/>`_
	* `Large Language Models: Scaling Laws and Emergent Properties <https://cthiriet.com/articles/scaling-laws>`_

.. seealso::
	* `Generative Agents: Interactive Simulacra of Human Behavior <https://arxiv.org/pdf/2304.03442.pdf>`_
	* `Locating and Editing Factual Associations in GPT <https://arxiv.org/pdf/2202.05262.pdf>`_
	* `Jarvis/HuggingGPT <https://github.com/microsoft/JARVIS>`_
	* `Sparks of Artificial General Intelligence <https://arxiv.org/pdf/2303.12712.pdf>`_
	* `Reflexion: an autonomous agent with dynamic memory and self-reflection <https://arxiv.org/pdf/2303.11366.pdf>`_
	* `Hyena Hierarchy: Towards Larger Convolutional Language Models <https://arxiv.org/pdf/2302.10866.pdf>`_
	* `Scaling Transformer to 1M tokens and beyond with RMT <https://arxiv.org/pdf/2304.11062.pdf>`_
	* `AI/ML/LLM/Transformer Models Timeline and List <https://ai.v-gar.de/ml/transformer/timeline/>`_
	* `Think Before You Act: Unified Policy for Interleaving Language Reasoning with Actions <https://arxiv.org/pdf/2304.11063.pdf>`_

******************************************************************************************
Applied LLMs
******************************************************************************************
.. note::
	* [Blog] `eugeneyan.com <https://eugeneyan.com/>`_
	* [Blog] `sh-reya.com <https://www.sh-reya.com/blog>`_
	* [Blog] `hamel.dev <https://hamel.dev/>`_
	* [Github] `LLM4Rec: Collection of papers <https://github.com/WLiK/LLM4Rec-Awesome-Papers>`_
	* [Github] Large Language Models for Generative Information Extraction: `Awesome-LLM4IE-Papers <https://github.com/quqxui/Awesome-LLM4IE-Papers>`_
	* [Github] Large Language Models Meet NLP: `Awesome-LLM-for-NLP <https://github.com/LightChen233/Awesome-LLM-for-NLP>`_
	* [Github] Knowledge graphs (KGs) and large language models (LLMs): `KG-LLM-Papers <https://github.com/zjukg/KG-LLM-Papers>`_
	* [Harvard] CS50 Tech Talk: `GPT-4 - How does it work, and how do I build apps with it? <https://www.youtube.com/watch?v=vw-KWfKwvTQ>`_

.. seealso::
	* `Freepik - A New Search for the New World <https://www.freepik.com/blog/new-search-new-world/>`_
	* `Replacing my best friends with an LLM <https://www.izzy.co/blogs/robo-boys.html>`_
	* `Become a 1000x engineer or die tryin <https://kadekillary.work/posts/1000x-eng/>`_
	* `Man and machine: GPT for second brains <https://reasonabledeviations.com/2023/02/05/gpt-for-second-brain/>`_
	* `Learn Prompting <https://learnprompting.org/>`_
	* `Prompt Engineering vs. Blind Prompting <https://mitchellh.com/writing/prompt-engineering-vs-blind-prompting>`_
	* `An example of LLM prompting for programming <https://martinfowler.com/articles/2023-chatgpt-xu-hao.html>`_
	* `Chat with any PDF <https://www.chatpdf.com/>`_
	* `AI prompt-to-storyboard videos w/ GPT, Coqui voices, StabilityAI images <https://meyer.id/>`_
	* `ChatGPT for your site <https://letterdrop.com/chatgpt?ref=hn>`_
	* `Web LLM runs the vicuna-7b Large Language Model entirely in your browser <https://simonwillison.net/2023/Apr/16/web-llm/>`_
	* [Paper] `AI Agents That Matter <https://arxiv.org/pdf/2407.01502>`_

******************************************************************************************
Must Read Papers
******************************************************************************************
.. csv-table:: 
	:header: "Tag", "Title"
	:align: center
	
		Attention,MHA: Attention Is All You Need
		Attention,MQA: Fast Transformer Decoding: One Write-Head is All You Need
		Attention,GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints
		Decoding,Fast Inference from Transformers via Speculative Decoding
		Activation,GLU Variants Improve Transformer
		Norm,Layer Normalization
		Norm,Root Mean Square Layer Normalization
		PE,RoFormer: Enhanced Transformer with Rotary Position Embedding
		MLM, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
		MLM, RoBERTa: A Robustly Optimized BERT Pretraining Approach
		MLM, TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Efficient Retrieval		
		RTD, Electra: Pre-training Text Encoders as Discriminators Rather Than Generators
		CLM, GPT2: Language Models are Unsupervised Multitask Learners
		CLM, GPT3: Language Models are Few-Shot Learners
		CLM, Finetuned Language Models Are Zero-Shot Learners
		CLM, LLaMA: Open and Efficient Foundation Language Models
		CLM, Llama 2: Open Foundation and Fine-Tuned Chat Models
		PLM, XLNet: Generalized Autoregressive Pretraining for Language Understanding
		GLM, GLM: General Language Model Pretraining with Autoregressive Blank Infilling
		MoE,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
		MoE, Mixtral: Mixtral of Experts
		Seq2Seq, BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation Translation and Comprehension
		Seq2Seq, T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
		Multilingual, XLM: Cross-lingual Language Model Pretraining
		Multilingual, XLM-R: Unsupervised Cross-lingual Representation Learning at Scale
		Multilingual, mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer
		Generalisation,Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets
		Scaling, Scaling Laws for Neural Language Models
		Scaling, Scaling Laws for Autoregressive Generative Modeling
		IR, E5: Text Embeddings by Weakly-Supervised Contrastive Pre-training
		IR, Dense Passage Retrieval for Open-Domain Question Answering
		LLM, Aligning language models to follow instructions
		LLM, Scaling Instruction-Finetuned Language Models
		LLM, InstructGpt: Training language models to follow instructions with human feedback
		LLM, Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning
		LLM, The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions
		LLM, PPO: Proximal Policy Optimization Algorithms
		LLM, SFT+RLHF: Learning to summarize from human feedback
		LLM, Reflexion: Language Agents with Verbal Reinforcement Learning
		LLM, RLCD: Reinforcement Learning from Contrastive Distillation for Language Model Alignment
		LLM, On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes
		LLM, DPO: Direct Preference Optimization: Your Language Model is Secretly a Reward Model
		LLM, D2PO: Discriminator-Guided DPO with Response Evaluation Models
		LLM, RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
		LLM, Large Language Models Are Latent Variable Models
		Representation, Scaling and evaluating sparse autoencoders
		Representation, Probabilistic Topic Modelling with Transformer Representations
		Representation, Matryoshka Representation Learning
		Representation, Not All Language Model Features Are Linear
		Context: Full, FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
		Context: Full, FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
		Context: Full, FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision
		Context: Sparse, Longformer: The Long-Document Transformer
		Context: Sparse, Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context
		Context, ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities
		Context, Data Engineering for Scaling Language Models to 128K Context
		Context, Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention
		Context, Ring Attention with Blockwise Transformers for Near-Infinite Context
		Context, Lost in the Middle: How Language Models Use Long Contexts
		Context, Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding
		Context, LongAlign: A Recipe for Long Context Alignment of Large Language Models
		Context, Long Context Transfer from Language to Vision
		Memory, MemoryBank: Enhancing Large Language Models with Long-Term Memory
		Memory, Augmenting Language Models with Long-Term Memory
		Memory, Recurrent Memory Transformer
		Memory, Scaling Transformer to 1M tokens and beyond with RMT
		Memory, Beyond Attention: Breaking the Limits of Transformer Context Length with Recurrent Memory
		Quant, LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale
		Quant, LoRA Low-Rank Adaptation of Large Language Models
		Quant, QLORA: Efficient Finetuning of Quantized LLMs
		Quant, SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models
		DiT, Scalable Diffusion Models with Transformers
		DiT, Scaling Rectified Flow Transformers for High-Resolution Image Synthesis
		ViT, Patch n' Pack: NaViT - a Vision Transformer for any Aspect Ratio and Resolution
		Eval, HELM: Holistic Evaluation of Language Models
		Hallucination, SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models
		Hallucination, A Survey of Hallucination in Large Foundation Models
		Hallucination, Hallucination Detection and Hallucination Mitigation: An Investigation
		Hallucination, MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection
		Hallucination, Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models
		Hallucination, To Believe or Not to Believe Your LLM

******************************************************************************************
Linear Algebra
******************************************************************************************

* [3Blue1Brown] `Essence of linear algebra <https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab>`_
* [MIT] `18.065 - Matrix Methods for Data Analysis <https://www.youtube.com/playlist?list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k>`_

******************************************************************************************
Calculus
******************************************************************************************

* [3Blue1Brown] `Essence of calculus <https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr>`_
* [Khan Academy] `Multivariable calculus <https://www.khanacademy.org/math/multivariable-calculus>`_
* [University of Victoria] `MATH200: Calculus III: Multivariable Calculus <https://www.youtube.com/playlist?list=PLHXZ9OQGMqxc_CvEy7xBKRQr6I214QJcd>`_

******************************************************************************************
Probability & Statistics
******************************************************************************************

* [MIT] `RES.6-012 Introduction to Probability <https://www.youtube.com/playlist?list=PLUl4u3cNGP60hI9ATjSFgLZpbNJ7myAg6>`_
* [CMU] `36-705 - Intermediate Statistics <https://www.youtube.com/playlist?list=PLt2Pd5kunvJ6-wpJG9hlWlk47c76bm9L6>`_
* [statisticsmatt] `Introduction to Mathematical Statistics with Illustrations using R <https://www.youtube.com/playlist?list=PLmM_3MA2HWpan-KlYp-QCbPHxMj5FK0TB>`_
* `Mathematician uncovers methods to shrink sampling errors in large-dimensional data sets <https://phys.org/news/2023-03-mathematician-uncovers-methods-sampling-errors.html>`_

******************************************************************************************
Analysis
******************************************************************************************

* [SO] `Pointwise vs. Uniform Convergence <https://math.stackexchange.com/questions/597765/pointwise-vs-uniform-convergence#915867>`_

******************************************************************************************
Complex Analysis
******************************************************************************************

* `Visual Complex Analysis <https://complex-analysis.com/content/table_of_contents.html>`_

******************************************************************************************
ML Theory
******************************************************************************************

* [Goodfellow] `Deep Learning <https://www.deeplearningbook.org/>`_
* [Roberts] `The Principles of Deep Learning Theory <https://arxiv.org/abs/2106.10165>`_
* [Kevin Murphy] `Probabilistic Machine Learning book1 <https://probml.github.io/pml-book/book1.html>`_
* [Kevin Murphy] `Probabilistic Machine Learning book2 <https://probml.github.io/pml-book/book2.html>`_
* [Bronstein,Bruna,Cohen,Veickovic][2021] `Geometric Deep Learning <https://geometricdeeplearning.com/>`_
* [Shwartz David] `Understanding Machine Learning - From Theory to Algorithms <https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf>`_
* [Mohri] `Foundations of Machine Learning <https://cs.nyu.edu/~mohri/mlbook/>`_
* [CMU] `11-785 Deep Learning <https://www.youtube.com/playlist?list=PLp-0K3kfddPxRmjgjm0P1WT6H-gTqE8j9>`_
* `Yet Another Derivation of Backpropagation in Matrix Form <https://sudeepraja.github.io/BackpropAdjoints/>`_
* `Gradients are Not All You Need <https://arxiv.org/pdf/2111.05803.pdf>`_
* `The Decade of Deep Learning <https://bmk.sh/2019/12/31/The-Decade-of-Deep-Learning/>`_
* `Long-Tailed Learning Requires Feature Learning <https://openreview.net/pdf?id=S-h1oFv-mq>`_
* `[MIT] Statistical Learning Theory and Applications <https://cbmm.mit.edu/lh-9-520/syllabus>`_
* `[GPSS] Gaussian Process Summer School <https://gpss.cc/gpss23/program>`_

******************************************************************************************
ML Practical
******************************************************************************************

* [Andrej Karpathy] `Neural Networks: Zero to Hero <https://karpathy.ai/zero-to-hero.html>`_
* `pytorch-internals <http://blog.ezyang.com/2019/05/pytorch-internals/>`_
* https://forums.fast.ai/t/diving-deep-into-pytorch/39470
* [Stevens] `Deep Learning with PyTorch <https://www.manning.com/books/deep-learning-with-pytorch>`_
* [Geron] `Hands-on Machine Learning <https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/>`_
* [Howard] `Deep Learning for Coders with fastai and PyTorch <https://course.fast.ai/Resources/book.html>`_
* [Zheng Casari] Feature Engineering for Machine Learning
* [NYU] `Deep Learning (Spring 2020) <https://atcold.github.io/pytorch-Deep-Learning/>`_
* [CMU] `Dive into Deep Learning <https://d2l.ai/index.html>`_
* [MIT] `6.S965 TinyML and Efficient Deep Learning <https://efficientml.ai/>`_
* [Microsoft Research] `LMOps <https://github.com/microsoft/LMOps>`_
* `Data Centric AI Cource <https://github.com/dcai-course/dcai-course>`_

******************************************************************************************
ML Design General Principle
******************************************************************************************

* [Andrew Ng] `Machine Learning Yearning <https://www.mlyearning.org/>`_
* [Chip Huyen] Designing Machine Learning Systems
* [Burkov] Machine Learning Engineering
* [Jeff Smith] Machine Learning Systems
* [Lakshmanan] Machine Learning Design Patterns
* [UCB] System Design for Large Scale Machine Learning

******************************************************************************************
ML Math
******************************************************************************************

* [Gutmann] Pen and Paper Exercise in ML
* `Steve Brunton Playlist <https://www.youtube.com/@Eigensteve/playlists>`_
* `Matrix Calculus <https://www.matrixcalculus.org/>`_

******************************************************************************************
ML Algorithms
******************************************************************************************

* [Naumann] The Art of Differentiating Computer Programs

******************************************************************************************
ML Related Theory
******************************************************************************************

* [MacKay] Information Throry Inference and Learning Algorithms
* [Brunton Kutz] Data Driven Science and Engineering
* [CUP] Probabilistic Numerics
* [Easley Kleinberg] Networks Crowds and Markets - Reasoning About a Highly Connected World
* `Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures <https://www.arxiv.org/abs/2407.09468>`_

******************************************************************************************
Applied ML
******************************************************************************************

* [Liu] Learning to Rank for Information Retrieval
* [MSR] A Short Introduction to Learning to Rank
* [MSR] LambdaMART
* [Ravichandiran] Getting Started with Google BERT
* [101ai.net] `BERT Explorer <https://www.101ai.net/text/bert>`_
* [Rothman] Transformers for Natural Language Processing
* [Tunstall] Natural Language Processing with Transformers
* [lilianweng] `The Transformer Family Version 2.0 <https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/>`_
* [Lakshmanan] Practical Machine Learning for Computer Vision
* Recent Advances and Trends in Multimodal Deep Learning
* Recommender Systems
* [Stanford] `CS224n: Natural Language Processing with Deep Learning <https://web.stanford.edu/class/cs224n/index.html>`_
* [Stanford] `CS224U - Natural Language Understanding <https://www.youtube.com/playlist?list=PLoROMvodv4rPt5D0zs3YhbWSZA8Q_DyiJ>`_
* [Stanford] `CS25 - Transformers United <https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM>`_
* [Stanford] `CS330 - Deep Multi-Task and Meta-Learning <https://www.youtube.com/playlist?list=PLoROMvodv4rMIJ-TvblAIkw28Wxi27B36>`_
* `From Deep to Long Learning? <https://hazyresearch.stanford.edu/blog/2023-03-27-long-learning>`_
* [CMU] `Graham Neubig's Teaching <https://www.phontron.com/teaching.php>`_
* [Princeton] `Against Predictive Optimization <https://predictive-optimization.cs.princeton.edu/>`_
* [Github] `Must Read Papers on Pre-Training <https://github.com/thunlp/PLMpapers>`_
* `NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers <https://speechresearch.github.io/naturalspeech2/>`_

******************************************************************************************
ML Papers
******************************************************************************************

* [dair-ai] `ML-Papers-Explained <https://github.com/dair-ai/ML-Papers-Explained>`_
* `Transformer models: an introduction and catalog — 2023 Edition <https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/>`_
* [Meta AI] `Teaching AI advanced mathematical reasoning <https://ai.facebook.com/blog/ai-math-theorem-proving/?utm_campaign=evergreen&utm_source=linkedin&utm_medium=organic_social&utm_content=blog>`_
* [Microsoft Research] `Why Can GPT Learn In-Context? <https://arxiv.org/pdf/2212.10559v2.pdf>`_
* [HM] `ML papers to implement <https://news.ycombinator.com/item?id=34503362>`_
* [ICLR2023] `Diffusion Models already have a Semantic Latent Space <https://arxiv.org/pdf/2210.10960.pdf>`_

******************************************************************************************
MLE Papers
******************************************************************************************

* [ACM] DNN for YouTube Recommendations
* [FB] Local Search
* [FB] Photo Search
* [FB] Recommeding items to more than a billion people
* [ICML] ScaNN
* [NeurIPS] DiskANN
* [KDD] Predicting Clicks on Ads at Facebook
* [RecSys] Recommending What Video to Watch Next
* `91% of ML Models Degrade in Time <https://www.nannyml.com/blog/91-of-ml-perfomance-degrade-in-time>`_

******************************************************************************************
MLOps
******************************************************************************************

* `The big dictionary of MLOps <https://www.hopsworks.ai/mlops-dictionary>`_

******************************************************************************************
ML Interviews
******************************************************************************************

* [Kashan] Deep Learning Interviews

******************************************************************************************
System Design General Principles
******************************************************************************************

* [Kleppmann] Designing Data-Intensive Applications
* [Alex Xu] System Design Interview - An Insiders Guide
* [Alex Xu] System Design Interview - An Insider's Guide Volume 2
* [Donne Matrin] `System Design Primer <https://github.com/donnemartin/system-design-primer>`_
* [Binh Nguyen] `Awesome Scalability <https://github.com/binhnguyennus/awesome-scalability>`_
* [Educative] `Grokking Modern System Design Interview for Engineers & Managers <https://www.educative.io/courses/grokking-modern-system-design-interview-for-engineers-managers>`_
* `A Senior Engineer's Guide to System Design Interview <https://interviewing.io/guides/system-design-interview>`_

******************************************************************************************
System Design Algorithms
******************************************************************************************

* [Gakhov] Probabilistic Data Structures and Algorithms
* [Tyler Neylon] `Introduction to Locality-Sensitive Hashing <https://tylerneylon.com/a/lsh1/lsh_post1.html>`_

******************************************************************************************
System Design Practical
******************************************************************************************

* `Build Your Own Redis with C/C++ <https://build-your-own.org/redis/>`_
* `Build Your Own Database <https://build-your-own.org/blog/20230420_byodb_done/>`_
* `The Inner Workings of Distributed Databases <https://questdb.io/blog/inner-workings-distributed-databases/>`_

******************************************************************************************
Layoffs
******************************************************************************************

* `Effective Immediately <https://github.com/Effective-Immediately/effective-immediately>`_

******************************************************************************************
Misc
******************************************************************************************

* `Sampling - Interesting post on LinkedIn <https://www.linkedin.com/posts/sahil0094_sampling-trainingdata-machinelearnig-activity-7043559310324285440-58h2>`_
* [Developer-Y] `CS Video Courses <https://github.com/Developer-Y/cs-video-courses>`_
* `Openintro Statistics <https://www.openintro.org/book/os/>`_
* `Demystifying Fourier analysis <https://dsego.github.io/demystifying-fourier/>`_
* `Data-oriented Programming in Python <https://www.moderndescartes.com/essays/data_oriented_python/>`_
* [CMU] `15-751 CS Theory Toolkit <https://www.youtube.com/playlist?app=desktop&list=PLm3J0oaFux3ZYpFLwwrlv_EHH9wtH6pnX>`_
* `Data Structure Sketches <https://okso.app/showcase/data-structures>`_
* [HN] `Vectors are over, hashes are the future <https://news.ycombinator.com/item?id=33123972>`_
* `Tensor Search <https://www.reddit.com/r/MachineLearning/comments/xk31n8/p_my_cofounder_and_i_quit_our_engineering_jobs_at/>`_
* `Philosophy of Mathematics - A Readinng List <https://www.logicmatters.net/2020/11/16/philosophy-of-mathematics-a-reading-list/>`_
* `The faker's guide to reading (x86) assembly language <https://www.timdbg.com/posts/fakers-guide-to-assembly/>`_
* `Learn C++ <https://www.learncpp.com/>`_
* `Introducing Austral: A Systems Language with Linear Types and Capabilities <https://borretti.me/article/introducing-austral>`_
* `A Beautiful Mathematical Reading List for 2023 <https://abakcus.com/a-beautiful-mathematical-reading-list-for-2023/>`_
* `Vector Animations With Python <https://zulko.github.io/blog/2014/09/20/vector-animations-with-python/>`_
* `Systems design 2: What we hope we know <https://apenwarr.ca/log/20230415>`_
* `Irregular Expressions <https://tavianator.com/2023/irregex.html>`_
* `The Prospect of an AI Winter <https://www.erichgrunewald.com/posts/the-prospect-of-an-ai-winter/>`_
* `When Will AI Take Your Job? <https://unchartedterritories.tomaspueyo.com/p/when-will-ai-take-your-job>`_
* `What Is Disruptive Innovation? <https://hbr.org/2015/12/what-is-disruptive-innovation>`_
* `Category Theory ∩ Machine Learning <https://github.com/bgavran/Category_Theory_Machine_Learning>`_
* `Building a Better World without Jobs <https://workforcefuturist.substack.com/p/building-a-better-world-without-jobs-video>`_
* `The Joy of Abstraction - An Introduction to Category Theory <https://johncarlosbaez.wordpress.com/2023/02/11/the-joy-of-abstraction/>`_
* `Clean Code - Horrible Performance <https://www.computerenhance.com/p/clean-code-horrible-performance>`_
* `Reverse Engineering a Mysterious UDP stream in my hotel <https://www.gkbrk.com/2016/05/hotel-music/>`_
* `Procrastinating is linked to health and career problems <https://theconversation.com/procrastinating-is-linked-to-health-and-career-problems-but-there-are-things-you-can-do-to-stop-188322>`_
* `Map of Reddit <https://anvaka.github.io/map-of-reddit/?v=2>`_
* `The Embedding Archives: Millions of Wikipedia Article Embeddings in Many Languages <https://txt.cohere.com/embedding-archives-wikipedia/>`_
* `Why Oatmeal is Cheap: Kolmogorov Complexity and Procedural Generation <https://knivesandpaintbrushes.org/projects/why-oatmeal-is-cheap/why_oatmeal_is_cheap_fdg2023.pdf>`_
* `Blog: Haskell in Production <https://serokell.io/blog/haskell-in-production>`_
* `How Does an FPGA Work? <https://learn.sparkfun.com/tutorials/how-does-an-fpga-work/all>`_
