###################################################################################
Gradient Descent
###################################################################################

.. note::
	* Local linear approximation information captured by gradient.
	* Local quadratic approximation information captured by Hessian.

***********************************************************************************
Nature of Stationary Point
***********************************************************************************
.. note::
	* Understanding the nature of local stationary point (maxima/minima/saddle point) with the help of Hessian.

.. warning::
	Largest eigenvalue = direction of slowest descent

***********************************************************************************
Approximating Gradient
***********************************************************************************
Stochastic Gradient Descent
===================================================================================
Mini-batch Gradient Descent
===================================================================================

***********************************************************************************
Learning-Rate Schedule
***********************************************************************************
.. warning::
	* With fixed learning rate - takes infinitely many steps to reach the minimum
	* TODO: proof

.. note::
	Larger LR at the beginning, smaller towards the end.

***********************************************************************************
Faster Covergence with Momentum
***********************************************************************************
.. note::
	Carry on a little bit extra along the previous direction before stopping and changing direction again.

Normal Momentum
===================================================================================
Nesterov Momentum
===================================================================================

***********************************************************************************
Adaptive Learning Rate
***********************************************************************************
.. note::
	Allow different LR along different Eigen-direction (essentially simulating Newton's Method)

AdaGrad
===================================================================================
RMSProp
===================================================================================
Adam
===================================================================================

***********************************************************************************
Managing Vanishing/Exploding Gradients
***********************************************************************************
Input normalisation
===================================================================================
Weight normalisation
===================================================================================
Batch Normalisation
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
Layer Normalisation
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
