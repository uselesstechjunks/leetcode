#########################################################################################
Natural Language Processing
#########################################################################################
.. warning::
	Goal: Write summary of key ideas and summary of papers

*****************************************************************************************
Tokenizers
*****************************************************************************************
WordPiece
=========================================================================================
SentencePiece
=========================================================================================
Byte-Pair Encoding (BPE)
=========================================================================================

*****************************************************************************************
Attention
*****************************************************************************************
Basic Attention
=========================================================================================
Self-Attention
=========================================================================================
Multi-Head Attention
=========================================================================================

*****************************************************************************************
Architecture
*****************************************************************************************
Encoder [BERT]
=========================================================================================
Decoder [GPT]
=========================================================================================
Encoder-Decoder [T5]
=========================================================================================

*****************************************************************************************
Training
*****************************************************************************************
Pretraining
=========================================================================================
Domain-Adaptation
=========================================================================================
Fine-Tuning
=========================================================================================
Choice of Loss Function
-----------------------------------------------------------------------------------------
Cross-Entropy
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Contrastive Loss
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
*****************************************************************************************
Special Techniques
*****************************************************************************************
Low-Rank Approximations (LoRA)
=========================================================================================
Layer Normalisation
=========================================================================================
Reinforcement Learning with Human Feedback (RLHF)
=========================================================================================

*****************************************************************************************
Task Specific Setup
*****************************************************************************************
Text Generation
=========================================================================================
Beam Search
-----------------------------------------------------------------------------------------
Text Classification
=========================================================================================
Finding Similar Items
=========================================================================================
Approximate Nearest Neighbour Search [DiskANN]
-----------------------------------------------------------------------------------------

*****************************************************************************************
Conversational Models
*****************************************************************************************
Prompt Engineering
=========================================================================================
Prompt Tuning
=========================================================================================

Resources
=========================================================================================
.. note::
	* `[HN] You probably donâ€™t need to fine-tune an LLM <https://news.ycombinator.com/item?id=37174850>`_
	* `[Ask HN] Most efficient way to fine-tune an LLM in 2024? <https://news.ycombinator.com/item?id=39934480>`_
	* `[HN] Finetuning Large Language Models <https://news.ycombinator.com/item?id=35666201>`_

		* `[magazine.sebastianraschka.com] Finetuning Large Language Models <https://magazine.sebastianraschka.com/p/finetuning-large-language-models>`_
